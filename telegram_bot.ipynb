{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vector_db = \"./data/spades/\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import sys\n",
    "import flask\n",
    "import telebot\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, LangchainEmbedding, PromptHelper, LLMPredictor, ServiceContext, set_global_service_context\n",
    "from llama_index import load_index_from_storage, StorageContext, QuestionAnswerPrompt\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.indices.postprocessor.node import SimilarityPostprocessor\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6599815538:AAE101fdtSmcMO5vMgjg8VTGcIgAl6-qlSc\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://10.12.1.33:8080\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Nov/2023 15:55:11] \"POST / HTTP/1.1\" 204 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the minimum setback\n",
      "2\n",
      "0\n",
      "data\\spades\\Setback_soup.html\n",
      "Setback_soup.html\n",
      "1\n",
      "data\\spades\\Setback_soup.html\n",
      "Setback_soup.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Nov/2023 15:56:23] \"POST / HTTP/1.1\" 204 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the minimum setback for a 10 storey that is near a road\n",
      "2\n",
      "0\n",
      "data\\spades\\Setback_soup.html\n",
      "Setback_soup.html\n",
      "1\n",
      "data\\spades\\Setback_soup.html\n",
      "Setback_soup.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Nov/2023 15:58:31] \"POST / HTTP/1.1\" 204 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the minimum setback for a 10 storey residential building that is near a category 1 road\n",
      "2\n",
      "0\n",
      "data\\spades\\Setback_soup.html\n",
      "Setback_soup.html\n",
      "1\n",
      "data\\spades\\Setback_soup.html\n",
      "Setback_soup.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Nov/2023 16:01:20] \"POST / HTTP/1.1\" 204 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the maximum site coverage for residential buildings\n",
      "2\n",
      "0\n",
      "data\\spades\\Site-Coverage_soup.html\n",
      "Site-Coverage_soup.html\n",
      "1\n",
      "data\\spades\\Maximum-Number-DU_soup.html\n",
      "Maximum-Number-DU_soup.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a document with filename in metadata\n",
    "document = Document(\n",
    "    text='text',\n",
    "    metadata={\n",
    "        'filename': '<doc_file_name>',\n",
    "        'category': '<category>'\n",
    "    }\n",
    ")\n",
    "\n",
    "document.metadata = {'filename': '<doc_file_name>'}\n",
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up GPT3.5-Turbo\n",
    "\n",
    "# set up path which is parent directory of this file\n",
    "path = os.getcwd()\n",
    "load_dotenv(f\"{path}/.env\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens= 100))\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "BOT_TOKEN = os.getenv(\"BOT_TOKEN\")\n",
    "print(BOT_TOKEN)\n",
    "\n",
    "# Prompt will be used to generate rich response,\n",
    "# Modify this prompt to add instruction\n",
    "\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"You are DoctorAI and an Ultrasound Physician Assistant. You will use information immediately to suggest an ultrasound imaging procedure appropriate for improving the overall assessment if necessary.\\n\"\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, Explain and add conclusion at the end: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "\n",
    "# Define a list of questions and their corresponding answers\n",
    "\n",
    "COMMON_QUESTIONS = {\n",
    "  \"Hello.\": \"Hi, I'm Doctor AI, your private medical advisor. The more details you provide about your symptoms, the more accurate advice I can offer. Please feel free to ask me any questions.\",\n",
    "  \"Hi.\": \"Hi, I'm Doctor AI, your private medical advisor. The more details you provide about your symptoms, the more accurate advice I can offer. Please feel free to ask me any questions.\"\n",
    "}\n",
    "\n",
    "# rebuild storage context\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=path_vector_db)\n",
    "# load index. Ensure service_context add here to use customized LLM\n",
    "index = load_index_from_storage(storage_context=storage_context,service_context=service_context)\n",
    "\n",
    "# add in prompt template and synthesizer...\n",
    "query_engine = index.as_query_engine(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=QA_PROMPT,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.75)],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "bot = telebot.TeleBot(BOT_TOKEN)\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# Process webhook calls\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def webhook():\n",
    "    if flask.request.headers.get('content-type') == 'application/json':\n",
    "        json_string = flask.request.get_data().decode('utf-8')\n",
    "#        logger.debug(f\"Request received: {json_string}\")\n",
    "\n",
    "        update = telebot.types.Update.de_json(json_string)\n",
    "        bot.process_new_updates([update])\n",
    "        return ('', 204)\n",
    "    else:\n",
    "        return ('Bad request', 400)\n",
    "\n",
    "@bot.message_handler(func=lambda message: True)\n",
    "def echo_message(message):\n",
    "\n",
    "    # Check if the user typed \"/start\"\n",
    "    if message.text == \"/start\":\n",
    "        welcome_message = '''\n",
    "Welcome to EduAI - Your 24/7 Education Advisor!\n",
    "\n",
    "We are thrilled to have you join us in this revolutionary educational journey. At EduAI, our mission is to provide expert educational support to anyone, anytime, anywhere.\n",
    "\n",
    "Powered by a state-of-the-art language model enriched with educational knowledge grounded by the relevant course notes, we aim to be your trusted companion for all your health-related inquiries.\n",
    "        '''\n",
    "        bot.reply_to(message, welcome_message)\n",
    "        return\n",
    "\n",
    "    # Check if the user typed \"/start\"\n",
    "    if message.text == \"/disclaimer\":\n",
    "\n",
    "        disclaimer_context = '''\n",
    "\n",
    "DoctorAI Disclaimer\n",
    "\n",
    "The information provided in response to patient questions on this Telegram channel is generated using a language model that incorporates content from the NHS website, which is publicly available and licensed under the Open Government Licence v3.0.\n",
    "\n",
    "Please note the following:\n",
    "\n",
    "Source Attribution: The information provided contains public sector information licensed under the Open Government Licence v3.0. It is derived from the NHS Website Content, but it is not directly cited from the NHS website or endorsed by the NHS specifically.\n",
    "Content Adaptation: Some of the information presented here may be adapted or modified as part of the language model's responses. While efforts have been made to ensure accuracy, changes in wording or context may occur, which could affect the original meaning or impact of the content.\n",
    "Risk and Responsibility: Any adaptation of NHS Website Content or use of non-refreshed NHS Website Content may invalidate its formal clinical approval.\n",
    "Liability: As the provider of this information, I do not bear responsibility for the accuracy, completeness, or validity of the responses generated by the language model.\n",
    "Seeking Professional Advice: The information provided here should not be considered a substitute for professional medical advice or consultation. If you have specific health concerns or require medical guidance, please consult a qualified healthcare professional or visit official NHS resources.\n",
    "Independent Verification: It is advisable to independently verify critical information from official NHS sources before making any decisions based on the responses provided on this channel.\n",
    "Personal Data: No personal data will be collected from the Telegram channel.\n",
    "\n",
    "By using this Telegram channel, you acknowledge and agree to the above disclaimer. If you do not agree, please refrain from using the information provided here.\n",
    "\n",
    "        '''\n",
    "\n",
    "        bot.reply_to(message, disclaimer_context)\n",
    "        return\n",
    "\n",
    "    query = message.text\n",
    "    print(query)\n",
    "\n",
    "    scores = {}\n",
    "    for question in COMMON_QUESTIONS:\n",
    "        score = fuzz.token_sort_ratio(query, question)\n",
    "        scores[question] = score\n",
    "    # Find the question with the highest match score\n",
    "    best_match = process.extractOne(query, COMMON_QUESTIONS.keys())\n",
    "\n",
    "    # If the match score is above a certain threshold, provide the corresponding answer\n",
    "    if best_match[1] > 75:\n",
    "        answer = COMMON_QUESTIONS[best_match[0]]\n",
    "        bot.reply_to(message, answer)\n",
    "        return\n",
    "\n",
    "    bot.reply_to(message, \"Message received. Please wait for response...\")\n",
    "\n",
    "    response = query_engine.query(query)\n",
    "    print(len(response.source_nodes))\n",
    "\n",
    "    response_ext = Markdown(f\"{response.response}\").data\n",
    "\n",
    "    response_ext = response_ext + \"\\n\\nReference used to form this response:\\n\"\n",
    "\n",
    "    # Assuming 'response' is the response from your query\n",
    "    for i, source_node in enumerate(response.source_nodes):\n",
    "        print(i)\n",
    "        filename = source_node.node.metadata.get(\"file_path\", None)\n",
    "        print(filename)\n",
    "        filename = filename.split('\\\\')[-1].replace('.txt', 'pdf')\n",
    "        print(filename)\n",
    "        # url = \"https://www.nhs.uk/conditions/\" + filename + \"/\"\n",
    "        response_ext = response_ext + filename + \"\\n\"\n",
    "\n",
    "    # print(response_ext)\n",
    "\n",
    "    if response_ext == \"None\":\n",
    "        response_ext = \"We were unable to find the answer in our current knowledge database. Kindly ask another question.\"\n",
    "\n",
    "    bot.reply_to(message, response_ext)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PORT = int(os.getenv(\"PORT\")) if os.getenv(\"PORT\") else 8080\n",
    "    app.run(host=\"0.0.0.0\", port=PORT, debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/spades/'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# # Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "# raw_documents = TextLoader('../../../state_of_the_union.txt').load()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# documents = text_splitter.split_documents(raw_documents)\n",
    "# db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(path_vector_db).load_data()\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1000)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "index.storage_context.persist(path_vector_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The setback for Category 1 road is 15m (inclusive of Green Buffer indicated in brackets).\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the setback for category 1 road\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = response.source_nodes[0].node.metadata.get(\"file_path\", None)\n",
    "filename = filename.split('/')[-1].replace('.txt', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ok\":true,\"result\":true,\"description\":\"Webhook was set\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "bot_token = os.getenv(\"BOT_TOKEN\")\n",
    "webhook_url = 'https://9ad0-202-94-70-51.ngrok-free.app'\n",
    "\n",
    "# Make an HTTP POST request to set the webhook\n",
    "response = requests.post(\n",
    "    f'https://api.telegram.org/bot{bot_token}/setWebhook',\n",
    "    data={'url': webhook_url}\n",
    ")\n",
    "\n",
    "# Print the response to check if it was successful\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
